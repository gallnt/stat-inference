\documentclass{article}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage[margin=1.5in]{geometry}

\DeclareMathOperator{\var}{Var}
\DeclareMathOperator{\corr}{Corr}

\title{Statistical Inference Chapter 2}
\author{Gallant Tsao}

\begin{document}

\maketitle

\begin{enumerate}
    % 2.1
    \item
    \begin{enumerate}
        % 1a
        \item Let $g(x)=x^3$. Then $g$ is monotonically increasing on $(0, 1)$. We get
        \[ 
        g^{-1}(y) = y^{1/3} \implies \frac{d}{dy}g^{-1}(y) = \frac{1}{3y^{2/3}}. 
        \]
        Since $X \in (0, 1), \ Y = X^3 \in (0, 1)$. Then by Theorem 2.1.5, 
        \begin{align*}
        f_{Y}(y) 
            &= f_{X}(g^{-1}(y)) \Bigl|\frac{d}{dy}g^{-1}(y)\Bigr| \\
            &= 42(y^{1/3})^{5}(1 - y^{1/3}) \cdot \frac{1}{3y^{2/3}} \\
            &= 14y(1 - y^{1/3}), \ y \in (0, 1).
        \end{align*}
        We also have 
        \begin{align*}
            \int_{0}^{1} 14y(1 - y^{1/3}) \ dy 
            &= 14\int_{0}^{1} y - y^{4/3} \ dy \\
            &= 14 \Big[ \frac{1}{2}y^2 - \frac{3}{7}y^{7/3} \Big]_{0}^{1} \\
            &= 14 ( \frac{1}{2} - \frac{3}{7}) \\
            &= 1.
        \end{align*}

        % 1b
        \item Let $g(x) = 4x + 3$. Then $g$ is monotonically increasing on $(0, \infty)$. We get 
        \[
        g^{-1}(y) = \frac{y - 3}{4} \implies \frac{d}{dy}g^{-1}(y) = \frac{1}{4}.
        \]
        Since $X \in  (0, \infty), \ Y = 4X + 3 \in (3, \infty)$. Then by Theorem 2.1.5,
        \begin{align*}
            f_{Y}(y)
            &= f_{X}(g^{-1}(y)) \Bigl|\frac{d}{dy}g^{-1}(y)\Bigr| \\
            &= 7e^{-7 \cdot \frac{y - 3}{4}} \cdot \frac{1}{4} \\
            &= \frac{7}{4}e^{\frac{21}{4} - \frac{7}{4}y}, \ y \in (3, \infty).
        \end{align*}
        We also have 
        \begin{align*}
            \int_{3}^{\infty} \frac{7}{4}e^{\frac{21}{4} - \frac{7}{4}y} \ dy 
            &= \frac{7}{4}e^{\frac{21}{4}} \int_{3}^{\infty} e^{-\frac{7}{4}y} \ dy \\
            &= \frac{7}{4}e^{\frac{21}{4}} \Big[ -\frac{4}{7}e^{-\frac{7}{4}y} \Big]_{3}^{\infty} \\
            &= \frac{7}{4}e^{\frac{21}{4}} (\frac{4}{7}e^{-\frac{21}{4}}) \\
            &= 1.
        \end{align*}

        % 1c
        \item Let $g(x) = x^2$. Then $g$ is monotonically increasing on $(0, 1)$. We get 
        \[
        g^{-1}(y) = \sqrt{y} \implies \frac{d}{dy}g^{-1}(y) = \frac{1}{2\sqrt{y}}.
        \]
        Since $X \in (0, 1), \ Y = X^2 \in (0, 1)$. Then by Theorem 2.1.5, 
        \begin{align*}
            f_{Y}(y) 
            &= f_{X}(g^{-1}(y)) \Bigl|\frac{d}{dy}g^{-1}(y)\Bigr| \\
            &= 30y(1 - \sqrt{y})^2 \cdot \frac{1}{2\sqrt{y}} \\
            &= 15\sqrt{y}(1 - \sqrt{y})^2, \ y \in (0, 1).
        \end{align*}
        We also have 
        \begin{align*}
            \int_{0}^{1} 15\sqrt{y}(1 - \sqrt{y})^2 \ dy
            &= 15 \int_{0}^{1} \sqrt{y} - 2y + y^{3/2} \ dy \\
            &= 15 \Big[ \frac{2}{3}y^{3/2} - y^2 + \frac{2}{5}y^{5/2} \Big]_{0}^{1} \\
            &= 15(\frac{2}{3} - 1 + \frac{2}{5}) \\
            &= 1.
        \end{align*}
        
    \end{enumerate}

    % 2.2
    \item 
    \begin{enumerate}
        \item Let $g(x) = x^2$. Then $g$ is monotonically increasing on $(0, 1)$. We get 
        \[
        g^{-1}(y) = \sqrt{y} \implies \frac{d}{dy}g^{-1}(y) = \frac{1}{2\sqrt{y}}.
        \]
        Since $X \in (0, 1), \ Y = X^2 \in (0, 1)$. Then by Theorem 2.1.5,
        \begin{align*}
            f_{Y}(y)
            &= f_{X}(g^{-1}(y)) \Bigl|\frac{d}{dy}g^{-1}(y)\Bigr| \\
            &= 1 \cdot \frac{1}{2\sqrt{y}} \\
            &= \frac{1}{2\sqrt{y}}, \ y \in (0, 1).
        \end{align*}

        \item Let $g(x) = -\log{x}$. Then $g$ is monotonically decreasing on $(0, 1)$. We get 
        \[
        g^{-1}(y) = e^{-y} \implies \frac{d}{dy}g^{-1}(y) = -e^{-y}.
        \]
        Since $X \in (0, 1), \ Y = \log{X} \in (0, \infty)$. Then by Theorem 2.1.5, 
        \begin{align*}
            f_{Y}(y)
            &= f_{X}(g^{-1}(y)) \Bigl|\frac{d}{dy}g^{-1}(y)\Bigr| \\
            &= \frac{(n + m + 1)!}{n!m!}e^{-ny}(1 - e^{-y})^{m} \cdot |-e^{-y}| \\
            &=  \frac{(n + m + 1)!}{n!m!}e^{-y(n + 1)}(1 - e^{-y})^{m}, \ y \in (0, \infty).
        \end{align*}

        \item Let $g(x) = e^{x}$. Then $g$ is monotonically increasing on $(0, \infty)$. We get 
        \[
        g^{-1}(y) = \ln{y} \implies \frac{d}{dy}g^{-1}(y) = \frac{1}{y}.
        \]
        Since $X \in (0, \infty), \ Y = e^{X} \in (0, \infty)$. Then by Theorem 2.1.5, 
        \begin{align*}
            f_{Y}(y)
            &= f_{X}(g^{-1}(y)) \Bigl|\frac{d}{dy}g^{-1}(y)\Bigr| \\
            &= \frac{1}{\sigma^2} \ln{y} e^{-(\ln{y}/\sigma)^2 /2} \cdot \frac{1}{y} \\
            &= \frac{1}{\sigma^2} \frac{\ln{y}}{y} e^{-(\ln{y}/\sigma)^2 / 2}, \ y \in (0, \infty).
        \end{align*}
    \end{enumerate}

    % 2.3
    \item First of all, 
    \[ X \in \{0, 1, 2, ...\} \implies Y \in \Big\{0, \frac{1}{2}, \frac{2}{3}, ...\Big\}. \]
    Then
    \begin{align*}
        P(Y = y) 
        &= P(\frac{X}{X + 1} = y) \\
        &= P(1 - \frac{1}{X + 1} = y) \\
        &= P(X = \frac{y}{1 - y}) \\
        &= \frac{1}{3} \Big( \frac{2}{3} \Big)^{y / (1 - y)}, \ y \in \Big\{\frac{k}{k + 1}: k \in 
        \mathbb{N}_0 \Big\}.
    \end{align*}
    
    % 2.4
    \item \begin{enumerate}
        \item It is not hard to see that $f(x) \geq 0 \ \forall x \in \mathcal{X}$ as both piecewise 
        functions are exponentials. We also have 
        \begin{align*}
            \int_{-\infty}^{\infty} f(x) \ dx 
            &= \int_{-\infty}^{0} \frac{1}{2}\lambda e^{\lambda x} 
            + \int_{0}^{\infty} \frac{1}{2}\lambda e^{-\lambda x} \ dx \\
            &= \Big[\frac{1}{2}e^{\lambda x} \Big]_{-\infty}^{0} + \Big[ -\frac{1}{2}e^{-\lambda x} 
            \Big]_{0}^{\infty} \\
            &= \frac{1}{2} + \frac{1}{2} \\
            &= 1.
        \end{align*}

        \item For $t \leq 0$, 
        \begin{align*}
            P(X < t) 
            &= \int_{-\infty}^{t} \frac{1}{2}\lambda e^{\lambda x} \ dx \\
            &= \Big[ \frac{1}{2}e^{\lambda x} \Big]_{\-\infty}^{t} \\
            &= \frac{1}{2}e^{\lambda t}.
        \end{align*}
        For $t > 0$,
        \begin{align*}
            P (X < t) 
            &= \frac{1}{2} + \int_{0}^{t} \frac{1}{2}\lambda e^{-\lambda x} \ dx \\
            &= \frac{1}{2} + \Big[ -\frac{1}{2}e^{-\lambda x} \Big]_{0}^{t} \\
            &= \frac{1}{2} + (-\frac{1}{2}e^{-\lambda t} + \frac{1}{2}) \\
            &= 1 - \frac{1}{2}e^{-\lambda t}.
        \end{align*}

        \item For $t \leq 0, \ P(|X| < t) = 0$. For $t > 0$,
        \begin{align*}
            P(|X| < t)
            &= P(-t < X < t) \\
            &= \int_{-t}^{0} \frac{1}{2}\lambda e^{\lambda x} \ dx 
            + \int_{0}^{t} \frac{1}{2}\lambda e^{-\lambda x} \ dx \\
            &= \Big[ \frac{1}{2}e^{\lambda x} \Big]_{-t}^{0} + \Big[ \frac{1}{2}e^{-\lambda x} 
            \Big]_{0}^{t} \\
            &= \frac{1}{2} - \frac{1}{2}e^{-\lambda t} + (-\frac{1}{2}e^{-\lambda t} + \frac{1}{2}) \\
            &= 1 - e^{-\lambda t}.
        \end{align*}
    \end{enumerate}

    % 2.5
    \item Let $A_0 = \{\pi\}, A_1 = (0, \frac{\pi}{2}), A_2 = (\frac{\pi}{2}, \pi), A_3 = (\pi, 
    \frac{3\pi}{2}), A_4 = (\frac{3\pi}{2}, 2\pi)$, and let $g(x) = g_i(x) = \sin^2{x}$. Then for each 
    $A_i (i \neq 0), g_i(x) = g(x) \forall x \in A_i,$, $g_i(x)$ is monotone on $A_i$. 
    Moreover, $\mathcal{Y} = (0, 1)$ is the same for all $i$, and monotone on $A_i$, and 
    \[
    g^{-1}(y) = \arcsin{(\sqrt{x})} \implies \frac{d}{dy}g^{-1}(y) = \frac{1}{2\sqrt{y(1 - y)}}
    \]
    is continuous on $\mathcal{Y}$ for all $i$. Then by Theorem 2.1.8,
    \begin{align*}
        f_{Y}(y) 
        &= \sum_{i = 1}^{4} f_{X}(g^{-1}(y)) \Big| \frac{d}{dy}g_{i}^{-1}(y) \Big| \\
        &= 4 \cdot \frac{1}{2\pi} \cdot \Big| \frac{1}{2\sqrt{y(1 - y)}} \Big| \\
        &= \frac{1}{\pi\sqrt{y(1 - y)}}, \ y \in (0, 1).
    \end{align*}
    To use the cdf from (2.1.6), we first get that $x_1 = \arcsin{(\sqrt{y})}, x_2 = \pi 
    - \arcsin{(\sqrt{y})}$. Note 
    \[ P(Y \leq y) = 2P(X \leq x_1) + 2P(X \leq \pi) - 2P(X \leq x_2) \]
    Then by differentiating the above we get 
    \begin{align*}
        f_{Y}(y)
        &= 2f_{X}(x_1) \cdot \frac{d}{dy}(\sin^{-1}{\sqrt{y}}) 
        - 2f_{X}(x_2) \cdot \frac{d}{dy}(\pi - \sin^{-1}{\sqrt{y}}) \\
        &= 2 \cdot \frac{1}{2\pi} \cdot \frac{1}{2\sqrt{y(1 - y)}} 
        - 2 \cdot \frac{1}{2\pi} \cdot (-\frac{1}{2\sqrt{y(1 - y)}}) \\
        &= \frac{1}{\pi\sqrt{y(1 - y)}}, \ y \in (0, 1).
    \end{align*}

    % 2.6
    \item \begin{enumerate}
        \item Let $g(x) = |x|^3, g_1(x) = -x^3, g_2(x) = x^3$. Let $A_0 = \{0\}, A_1 = (-\infty, 0), 
        A_2 = (0, \infty)$. Then we get $\mathcal{Y} = (0, \infty)$ so that all conditions for Theorem 2.1.8 
        are satisfied. Then 
        \[ g_{1}^{-1}(y) = -y^{1/3} \implies \frac{d}{dy}g_{1}^{-1}(y) = -\frac{1}{3y^{2/3}}. \]
        \[ g_{2}^{-1}(y) = y^{1/3} \implies \frac{d}{dy}g_{2}^{-1}(y) = \frac{1}{3y^{2/3}}. \]        
        Then by Theorem 2.1.8,
        \begin{align*}
            f_{Y}(y) 
            &= \sum_{i = 1}^{2} f_{X}(g^{-1}(y)) \Big| \frac{d}{dy}g_{i}^{-1}(y) \Big| \\
            &= \frac{1}{2}e^{-y^{1/3}} \cdot \Big| -\frac{1}{3y^{2/3}} \Big|
            + \frac{1}{2}e^{-y^{1/3}} \cdot \Big| \frac{1}{3y^{2/3}} \Big| \\
            &= \frac{1}{3}y^{-2/3}e^{-y^{1/3}}, \ y \in (0, \infty).
        \end{align*}

        \item Let $g(x) = g_1(x) = g_2(x) = 1 - x^2$. Let $A_0 = \{0\}, A_1 = (-1, 0), A_2 = (0, 1)$. Then 
        we get 
        \[ g_{1}^{-1}(y) = -\sqrt{1 - y} \implies \frac{d}{dy}g_{1}^{-1}(y) = \frac{1}{2\sqrt{1 - y}}, \]
        \[ g_{2}^{-1}(y) = \sqrt{1 - y} \implies \frac{d}{dy}g_{2}^{-1}(y) = -\frac{1}{2\sqrt{1 - y}}. \]
        Then we get $\mathcal{Y} = (0, 1)$ so that all conditions for Theorem 2.1.8 are satisfied. Then by 
        Theorem 2.1.8, 
        \begin{align*}
            f_{Y}(y)
            &= \sum_{i = 1}^{2} f_{X}(g^{-1}(y)) \Big| \frac{d}{dy}g_{i}^{-1}(y) \Big| \\
            &= \frac{3}{8}(-\sqrt{1 - y} + 1)^2 \cdot \Big| \frac{1}{2\sqrt{1 - y}} \Big| \\
            &\qquad \qquad + \frac{3}{8}(\sqrt{1 - y} + 1)^2 \cdot \Big| -\frac{1}{2\sqrt{1 - y}} \Big| \\
            &= \frac{3}{8} (1 - y - 2\sqrt{1 - y} + 1) \cdot \frac{1}{2\sqrt{1 - y}} \\
            &\qquad \qquad + \frac{3}{8} ( 1 - y + 2\sqrt{1 - y} + 1) \cdot \frac{1}{2\sqrt{1 - y}} \\
            &= \frac{3}{8}(1 - y)^{1/2} + \frac{3}{8}(1 - y)^{-1/2}, \ y \in (0, 1).
        \end{align*}
        (Note for $g_1$ we chose the negative root because $x < 0$).

        \item Let $g_1(x) = 1 - x^2, g_2(x) = 1 - x$. Let $A_0 = \{0\}, A_1 = (-1, 0), A_2 = (0, 1)$. Then 
        we get 
        \[ g_{1}^{-1}(y) = -\sqrt{1 - y} \implies \frac{d}{dy}g_{1}^{-1}(y) = \frac{1}{2\sqrt{1 - y}}. \]
        \[ g_{2}^{-1}(y) = 1 - y \implies \frac{d}{dy}g_{2}^{-1}(y) = -1. \]
        Then we get $\mathcal{Y} = (0, 1)$ so that all conditions for Theorem 2.1.8 are satisfied. Then by 
        Theorem 2.1.8, 
        \begin{align*}
            f_{Y}(y)
            &= \sum_{i = 1}^{2} f_{X}(g^{-1}(y)) \Big| \frac{d}{dy}g_{i}^{-1}(y) \Big| \\
            &= \frac{3}{8}(-\sqrt{1 - y} + 1)^2 \cdot \Big| \frac{1}{2\sqrt{1 - y}} \Big| \\
            &\qquad \qquad + \frac{3}{8} (1 - y + 1)^2 \cdot |-1| \\
            &= \frac{3}{16\sqrt{1 - y}}(1 - \sqrt{1 - y})^2 + \frac{3}{8}(2 - y)^2, \ y \in (0, 1).
        \end{align*}
    \end{enumerate}

    % 2.7
    \item \begin{enumerate}
        \item For $g(x)=x^2, \ x \in [-1, 2]$, there is no partition $\{A_i\}$ of the interval which could 
        produce the same $\mathcal{Y}$ for all $i$. Therefore, we cannot use Theorem 2.1.8 in this case. 
        To solve directly, we get 
        \begin{align*}
            f_{Y}(y) 
            &= \sum_{i = 1}^{4} f_{X}(g^{-1}(y)) \Big| \frac{d}{dy}g_{i}^{-1}(y) \Big| \\
            &= 
        \end{align*}
    \end{enumerate}

    % 2.8
    \item \begin{enumerate}
        \item It is easy to see that 
        \[ \lim_{x \to -\infty} F_{X}(x) = 0, \ \lim_{x \to +\infty} F_{X}(x) = 1. \]
        Moreover, both $0$ and $1-e^{-x}$ are non-decreasing on their respective intervals, and 
        \[ \lim_{x \to 0^+} F_{X}(x) = 0 \]
        so that $F_X$ is right continuous and therefore is a valud cdf. Its inverse is 
        \[ F_{X}^{-1}(y) = -\ln{(1 - y)}\]

        \item Again, we can see that 
        \[ \lim_{x \to -\infty} F_{X}(x) = 0, \ \lim_{x \to +\infty} F_{X}(x) = 1. \]
        $e^{x} / 2, 1 - (e^{-x} / 2)$ are increasing, and $1/2$ is noncreasing on their respective intervals, 
        and 
        \[ \lim_{x \to 0} F_{X}(x) = \frac{1}{2}, \ \lim_{x \to 1} F_{X}(x) = \frac{1}{2} \]
        so that $F_X$ is continuous hence right continuous so is a valid cdf. Its inverse is
        \[ F_{X}^s{-1}(y) = \begin{cases}
            \ln{(2x)} & 0 < y < \frac{1}{2} \\
            -\ln{(2-2x)} & \frac{1}{2} \leq y < 1.
        \end{cases} \]

        \item Again, we can see that 
        \[ \lim_{x \to -\infty} F_{X}(x) = 0, \ \lim_{x \to +\infty} F_{X}(x) = 1. \]
        $e^{x}/4, 1 - (e^{-x} / 4)$ are both increasing on their respective intervals, and 
        \[ \lim_{x \to 0^+} F_{X}(x) = \frac{3}{4} = F_{X}(0) \]
        so that $F_X$ is right continuous and therefore is a valid cdf. Its inverse is 
        \[ F_{X}^{-1}(y) = \begin{cases}
            \ln{(4x)} & 0 < y < \frac{1}{4} \\
            -\ln{(4-4x)} & \frac{3}{4} \leq y < 1
        \end{cases}\]
    \end{enumerate}

    % 2.9
    \item We first find the cdf of $X$:
    \[ F_{X}(x) = \begin{cases}
        0 & x \leq 1 \\
        \frac{1}{4}(x - 1)^2 & 1 < x < 3 \\
        1 & x \geq 3
    \end{cases} \]
    Then we have 
    \[ \lim_{x \to 1} F_{X}(x) = 0, \ \lim_{x \to 3} F_{X}(x) = 1. \]
    hence $X$ has a continuous cdf. Let $u(x) = F_{X}(x)$. Then $u(x)$ is nondecreasing and by 
    Theorem 2.1.10, $Y = u(X)$ has a uniform distribution.

    % 2.10
    \item \begin{enumerate}
        \item 
    \end{enumerate}

    % 2.11
    \item \begin{enumerate}
        \item \begin{align*}
            \mathbb{E}[X^2]
            &= \int_{-\infty}^{\infty} x^2 \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}} \\
            &= \frac{1}{\sqrt{2\pi}} \Big( [-xe^{-\frac{x^2}{2}}]_{-\infty}^{+\infty} 
            + \int_{-\infty}^{+\infty} e^{-\frac{x^2}{2}} \ dx \Big) \\
            &= \frac{1}{\sqrt{2\pi}} \cdot \sqrt{2\pi} \\
            &= 1.
        \end{align*}
        From Example 2.1.7, 
        \begin{align*}
            f_{Y}(y) 
            &= \frac{1}{2\sqrt{y}} (f_{x}(\sqrt{y}) + f_{X}(-\sqrt{y})) \\
            &= \frac{1}{2\sqrt{y}} \Big( \frac{1}{\sqrt{2\pi}} e^{-\frac{y}{2}} 
            + \frac{1}{\sqrt{2\pi}} e^{-\frac{y}{2}} \Big) \\
            &= \frac{1}{\sqrt{2\pi}} \frac{1}{\sqrt{y}} e^{-\frac{y}{2}}, \quad y > 0.
        \end{align*}
        Using integration by parts, 
        \begin{align*}
            \mathbb{E}[Y]
            &= \int_{0}^{\infty} y \frac{1}{\sqrt{2\pi}} \frac{1}{\sqrt{y}} 
            e^{-\frac{y}{2}} \ dy \\
            &= \frac{1}{\sqrt{2\pi}} \int_{0}^{\infty} \sqrt{y} e^{-\frac{y}{2}} \ dy \\
            &= \frac{1}{\sqrt{2\pi}} \Big( [-2\sqrt{y}e^{-\frac{y}{2}}]_{0}^{\infty} 
            + \int_{0}^{\infty} \frac{1}{\sqrt{y}}e^{-\frac{y}{2}}) \ dy \\
            &= \frac{1}{\sqrt{2\pi}} \cdot \sqrt{2\pi} \\
            &= 1.
        \end{align*}
        (Note that the term on the right is the kernel of the Chi-squared distribution 
        defined in Example 2.1.9 earlier.)

        \item We first find the cdf of $Y$.
        \[ F_{Y}(y) = P(|X| \leq y) = P(-y \leq X \leq y) = F_{X}(y) - F_{X}(-y). \]
        Therefore the pdf of $Y$ is just 
        \begin{align*}
            f_{Y}(y) \\
            &= f_{X}(y) + f_{X}(-y) \\
            &= \frac{1}{\sqrt{2\pi}} e^{-\frac{y^2}{2}} 
            + \frac{1}{\sqrt{2\pi}} e^{-\frac{y^2}{2}} \\
            &= \sqrt{\frac{2}{\pi}} e^{-\frac{y^2}{2}}, \quad y \in [0, \infty).
        \end{align*}
        Therefore we can find the mean and variance of $Y$: 
        \begin{align*}
            \mathbb{E}[Y]
            &= \int_{0}^{\infty} y \sqrt{\frac{2}{\pi}} e^{-\frac{y^2}{2}} \\
            &= \sqrt{\frac{2}{\pi}} [-e^{-\frac{y^2}{2}}]_{0}^{\infty} \\
            &= \sqrt{\frac{2}{\pi}}.
        \end{align*}
        From part (a), 
        \[ \mathbb{E}[Y^2] = \mathbb{E}[|X|^2] = \mathbb{E}[X^2] = 1. \]
        Therefore, 
        \[ \var(Y) = \mathbb{E}[Y^2] - (\mathbb{E}[Y])^2 = 1 - \frac{2}{\pi}. \]
    \end{enumerate}

    % 2.12
    \item We have that $X \sim \text{Uniform}(0, \frac{pi}{2})"$ and $Y = d\tan{X}$. 
    Ket $g(x) = d\tan{x}$. Then $g$ is increasing on $(0, \frac{\pi}{2})$. For 
    $X \in (0, \frac{\pi}{2}), \quad Y \in (0, \infty)$. We have that 
    $g{-1}(y) = \arctan{y/d}$ has a continuous derivative on $(0, \infty)$. Then by 
    Theorem 2.1.5, 
    \begin{align*}
        f_{Y}(y) 
        &= f_{X}(g^{-1}(y)) \Big| \frac{d}{dy}g^{-1}(y) \Big| \\
        &= \frac{2}{\pi} \frac{1}{1 + (y/d)^2} \cdot \frac{1}{d} \\
        &= \frac{2}{\pi d} \frac{1}{1 + (y/d)^2}, \quad y \in (0, \infty).
    \end{align*}
    Then $Y \sim \text{Cauchy}(0, d)$ so therefore $\mathbb{E}[Y] = \infty$.

    % 2.13
    \item We have that For $X = k$, we can either have $k$ tails followed by a head or $k$ 
    heads followed by a tail. Then 
    \[ P(X = k) = (1 - p)^{k}p + p^{k}(1 - p), \quad k = 1, 2, ... \]
    Then 
    \begin{align*}
        \mathbb{E}[X] 
        &= \sum_{k = 1}^{\infty} k[(1 - p)^{k}p + p^{k}(1 - p)] \\
        &= p(1 - p) \sum_{k = 1}^{\infty} k(1 - p)^{k - 1}p + kp^{k - 1}(1 - p) \\
        &= p(1 - p) \Big( \frac{1}{p^2} + \frac{1}{(1 - p)^2}) \\
        &= \frac{1 -2p + 2p^2}{p(1 - p)}.
    \end{align*}

    % 2.14
    \item \begin{enumerate}
        \item \begin{align*}
            \mathbb{E}[X]
            &= \int_{0}^{\infty} xf_{X}(x) \ dx \\
            &= [xF_{X}(x)]_{0}^{\infty} - \int_{0}^{\infty} F_{X}(x) \ dx
        \end{align*}
    \end{enumerate}

    % 2.15
    \item We can assume without loss of generality that $X \leq Y$ as the other case is similar. Then 
    $X \wedge Y = X, X \vee Y = Y$. Taking expectations on both sides gives the result.

    % 2.16
    \item From Exercise 2.14, 
    \begin{align*}
        \mathbb{E}[T]
        &= \int_{0}^{\infty} ae^{-\lambda t} + (1 - a)e^{-\mu t} \ dt \\
        &= \Bigl[ -\frac{a}{\lambda}e^{-\lambda t} + \frac{a - 1}{\mu}e^{-\mu t} \Bigr]_{0}^{\infty} \\
        &= \frac{a}{\lambda} + \frac{1 - a}{\mu}.
    \end{align*}

    % 2.17
    \item \begin{enumerate}
        \item \[
        \int_{0}^{m} 3x^2 = [x^3]_{0}^{m} = m^3 = \frac{1}{2} \implies m = \frac{1}{\sqrt[3]{2}}.
        \]

        \item This is the pdf of the standard Cauchy distribution, which has median 0.
    \end{enumerate}

    % 2.18
    \item \begin{align*}
        \mathbb{E}[|X - a|]
        &= \int_{-\infty}^{\infty} |x - a| f_{X}(x) \ dx \\
        &= \int_{-\infty}^{a} -(x - a)f_{X}(x) \ dx + \int_{a}^{\infty} (x - a)f_{X}(x) \ dx.
    \end{align*}
    Taking the derivative with respect to $a$, 
    \[ \frac{d}{da} \mathbb{E}[|X - a|] 
    = \int_{-\infty}^{a} f_{X}(x) \ dx - \int_{a}^{\infty} f_{X}(x) \ dx. \]
    Setting the above to 0 yields that $a$ is the median. By the second derivative test, 
    \[ \frac{d^2}{da^2} \mathbb{E}[|X - a|] = 2f(a) > 0 \]
    so that we have a minimum.

    % 2.19
    \item 

    % 2.20
    \item Let $X$ be the number of children until the first daughter. Then $X \sim \text{Geom}(p)$. Then 
    $\mathbb{E}[X] = \frac{1}{p}$.

    % 2.21
    \item Since $y = g(x)$ and $g(x)$ is monotone, $x = g^{-1}(y) \implies dx = \frac{d}{dy}g^{-1}(y) dy$. 
    \begin{align*}
        \mathbb{E}[g(X)]
        &= \int_{\infty}^{\infty} g(x)f_{X}(x) \ dx \\
        &= \int_{-\infty}^{\infty} g(g^{-1}(y)) f_{X}(g^{-1}(y)) \frac{d}{dy}g^{-1}(y) \ dy \\
        &= \int_{-\infty}^{\infty} yf_{Y}(y) \ dy \\
        &= \mathbb{E}[Y].
    \end{align*}

    % 2.22
    \item \begin{enumerate}
        \item It is clear that $f(x) > 0$ when $0 < x < \infty$. In here we will just calculate the kernel 
        and show that it is the reciprocal of $\frac{4}{\beta^{3} \sqrt{\pi}}$.
        \begin{align*}
            \int_{0}^{\infty} x^{2} e^{-x^2 / \beta^2} \ dx 
            &= \Bigl[ -\frac{\beta^2}{2}xe^{-x^2 / \beta^2} \Bigr]_{0}^{\infty}
            + \int_{0}^{\infty} \frac{\beta^{2}}{2} e^{-x^2 / \beta^2} \ dx \\
            &= 0 + \int_{0}^{\infty} \frac{\beta^3}{4} e^{-u^2} \ du \quad (u = \frac{x}{\beta}) \\
            &= \frac{\beta^3 \sqrt{\pi}}{4},
        \end{align*}
        which is correct.

        \item Using integration by parts, 
        \begin{align*}
            \mathbb{E}[X]
            &= \frac{4}{\beta^3 \sqrt{\pi}} \int_{0}^{\infty} x^3 e^{-x^2 / \beta^2} \ dx \\
            &= \frac{4}{\beta^3 \sqrt{\pi}} \Biggl[ \Bigl[ -\frac{\beta^2}{2}x^2 e^{-x^2 / \beta^2} 
            \Bigr]_{0}^{\infty} + \beta^2 \int_{0}^{\infty} xe^{-x^2 / \beta^2} \ dx \Biggr] \\
            &= \frac{4}{\beta^3 \sqrt{\pi}} \Bigl[ 0 + \beta^2 
            [-\frac{\beta^2}{2}e^{-x^2 / \beta^2}]_{0}^{\infty} \Bigr] \\
            &= \frac{2\beta}{\sqrt{\pi}}. \\
            \mathbb{E}[X^2]
            &= \frac{4}{\beta^3 \sqrt{\pi}} \int_{0}^{\infty} x^4 e^{-x^2 / \beta^2} \ dx \\
            &= \frac{4}{\beta^3 \sqrt{\pi}} \Biggl[ \Bigl[ -\frac{\beta^2}{2}x^3 e^{-x^2 / \beta^2} 
            \Bigr]_{0}^{\infty} + \frac{3\beta^2}{2} \int_{0}^{\infty} x^2e^{-x^2 / \beta^2} \ dx \Biggr] \\
            &= \frac{4}{\beta^3 \sqrt{\pi}} \Bigl( 0 + \frac{3\beta^2}{2} \cdot 
            \frac{\beta^3 \sqrt{\pi}}{4} \Bigr) \\
            &= \frac{3\beta^2}{2}. \\
            \var{X}
            &= \frac{3\beta^2}{2} - \Bigl( \frac{2\beta}{\sqrt{\pi}} \Bigr)^2 \\
            &= \beta^2 \Bigl( \frac{3}{2} - \frac{4}{\pi} \Bigr).
        \end{align*}
    \end{enumerate}

    % 2.23
    \item \begin{enumerate}
        \item First of all, $X \in (-1, 1)$ hence $Y = X^2 \in [0, 1)$.
        \begin{align*}
            F_{Y}(y)
            &= P(X^2 \leq y) \\
            &= P(-\sqrt{y} \leq X \leq \sqrt{y}) \\
            &= F_{X}(\sqrt{y}) - F_{X}(-\sqrt{y}).
        \end{align*}
        Taking derivatives, 
        \begin{align*}
            f_{Y}(y) 
            &= f_{X}(\sqrt{y}) \cdot \frac{1}{2\sqrt{y}} - f_{X}(-\sqrt{y}) \cdot 
            \Bigl( -\frac{1}{2\sqrt{y}} \Bigr) \\
            &= \frac{1}{2}(1 + \sqrt{y}) \cdot \frac{1}{2\sqrt{y}} + \frac{1}{2}(1 - \sqrt{y}) \cdot
            \Bigl( \frac{1}{2\sqrt{y}} \Bigr) \\
            &= \frac{1}{2}y^{-1 / 2}, \ y \in (0, 1).
        \end{align*}

        \item \begin{align*}
            \mathbb{E}[Y]
            &= \int_{0}^{1} \frac{1}{2}\sqrt{y} \ dy \\
            &= \Bigl[ \frac{1}{3}y^{3/2} \Bigr]_{0}^{1} \\
            &= \frac{1}{3}. \\
            \mathbb{E}[Y^2]
            &= \int_{0}^{1} \frac{1}{2}y^{3 / 2} \ dy \\
            &= \Bigl[ \frac{1}{5}y^{5 / 2} \Bigr]_{0}^{1} \\
            &= \frac{1}{5}. \\
            \var{Y}
            &= \frac{1}{5} - \Bigl( \frac{1}{3} \Bigr)^2 \\
            &= \frac{4}{45}.
        \end{align*}
    \end{enumerate}

    % 2.24
    \item \begin{enumerate}
        \item \begin{align*}
            &\mathbb{E}[X]
            = \int_{0}^{1} ax^a \ dx
            = \Bigl[ \frac{a}{a + 1} x^{a + 1} \Bigr]_{0}^{1}
            = \frac{a}{a + 1}. \\
            &\mathbb{E}[X^2]
            = \int_{0}^{1} ax^{a + 1} \ dx 
            = \Bigl[ \frac{a}{a + 2} x^{a + 2} \Bigr]_{0}^{1} 
            = \frac{a}{a + 2}. \\
            &\var{X} 
            = \frac{a}{a + 2} - \Bigl( \frac{a}{a + 1} \Bigr)^{2} 
            = \frac{a}{(a + 2)(a + 1)^2}.
        \end{align*}

        \item \begin{align*}
            &\mathbb{E}[X] 
            = \sum_{k = 1}^{n} \frac{k}{n} 
            = \frac{n(n + 1)}{2n} 
            = \frac{n + 1}{2}. \\
            &\mathbb{E}[X^2] 
            = \sum_{k = 1}^{n} \frac{k^2}{n} 
            = \frac{n(n + 1)(2n + 1)}{6n}
            = \frac{(n + 1)(2n + 1)}{6}. \\
            &\var{X} 
            = \frac{(n + 1)(2n + 1)}{6} - \Bigl( \frac{n + 1}{2} \Bigr)^2 
            = \frac{n^2 + 1}{12}.
        \end{align*}

        \item \begin{align*}
            &\mathbb{E}[X] 
            = \frac{3}{2}\int_{0}^{2} x^3 - 2x^2 + x \ dx 
            = \frac{3}{2} \Bigl[ \frac{1}{4}x^4 - \frac{2}{3}x^3 + \frac{1}{2}x^2 \Bigr]_{0}^{2} 
            = 1. \\
            &\mathbb{E}[X^2] 
            = \frac{3}{2}\int_{0}^{2} x^4 - 2x^3 + x^2 \ dx 
            = \frac{3}{2} \Bigl[ \frac{1}{5}x^5 - \frac{1}{2}x^4 + \frac{1}{3}x^3 \Bigr]_{0}^{2} 
            = \frac{8}{5}. \\
            &\var{X} 
            = \frac{8}{5} - 1^2 
            = \frac{3}{5}.
        \end{align*}
    \end{enumerate}

    % 2.25
    \item \begin{enumerate}
        \item Let $Y = -X$. Then $g(x) = g^{-1}(x) = -x$. We have that 
        \[ f_{-X}(x) = f_{X}(-x) \cdot |-1| = f_{X}(x) \forall x \]
        so that $X$ and $-X$ are identically distributed.

        \item Let $\varepsilon > 0$ be given. Then 
        \begin{align*}
            M_X(0 + \varepsilon)
            &= \int_{-\infty}^{\infty} e^{\varepsilon x} f_X(x) \ dx \\
            &= -\int_{\infty}^{-\infty} e^{(-\varepsilon u)} f_X(u) \ du \quad (u = -x) \\
            &= \int_{-\infty}^{\infty} e^{(0 - \varepsilon)u} f_X(u) \ du \\
            &= M_X(0 - \varepsilon).
        \end{align*}
        Since $\varepsilon > 0$ is arbitrary, we are done.
    \end{enumerate}

    % 2.26
    \item \begin{enumerate}
        \item $N(\mu, \sigma^2)$ is symmetric about $\mu$, $\text{DoubleExp}(\mu, b)$ is 
        symmetric about $\mu$, and $t_n$ is symmetric about 0.

        \item \begin{align*}
            \int_{a}^{\infty} f(x) \ dx
            &= \int_{0}^{\infty} f(a + \varepsilon) \ d\varepsilon \quad (\varepsilon = x - a) \\
            &= \int_{0}^{\infty} f(a - \varepsilon) \ d\varepsilon \\
            &= \int_{a}^{\infty} f(x) \ dx \quad (x = a + \varepsilon)
        \end{align*}
        Since $f$ is a valid pdf, $a$ has to be the median.

        \item \begin{align*}
            \mathbb{E}[X] - a
            &= \mathbb{E}[X = a] \\
            &= \int_{-\infty}^{\infty} (x - a)f(x) \ dx \\
            &= \int_{-\infty}^{a} (x - a)f(x) \ dx + \int_{a}^{\infty} (x - a)f(x) \ dx \\
            &= \int_{0}^{\infty} -\varepsilon f(a - \varepsilon) \ d\varepsilon 
            + \int_{0}^{\infty} \varepsilon f(a + \varepsilon) \ d\varepsilon \\
            &= -\int_{0}^{\infty} \varepsilon f(a + \varepsilon) \ d\varepsilon 
            + \int_{0}^{\infty} \varepsilon f(a + \varepsilon) \ d\varepsilon \\
            &= 0.
        \end{align*}
        Here, we substituted $\varepsilon = a - x$ for the first integral and $\varepsilon = x - a$ for the 
        second integral (sorry for the confusing notation).

        \item If $a < 0$, for $\varepsilon > a$, $f(a - \varepsilon) = 0$ but $f(a + \varepsilon) > 0$. 
        If $a \geq 0$, the same is true, hence $f(x)$ is not a symmetric pdf.

        \item For the mean, 
        \begin{align*}
            \mathbb{E}[X]
            &= \int_{0}^{\infty} xe^{-x} \ dx \\
            &= [-xe^{-x} - e^{-x}]_{0}^{\infty} \\
            &= 1.
        \end{align*}
        For the median, 
        \[ \int_{0}^{a} e^{-x} = \frac{1}{2} \implies a = \log{2}. \]
        Since $\log{2} < 1$, the median is less than the mean.
    \end{enumerate}

    % 2.27
    \item \begin{enumerate}
        \item The standard normal has a unique mode at $x = 0$.
        
        \item The $\text{Uniform}(0, 1)$ does not have a unique mode as all $x \in (0, 1)$ is a mode.
        
        \item First suppose that the mode is unique. Let $a$ be the mean and $b$ be the mode suppose that 
        $a \neq b$. We can assume without loss of generality that $a = b + \varepsilon$. Since $f(x)$ is 
        unimodal, $f(b) > f(b + \varepsilon) \geq f(b + 2\varepsilon)$, and 
        $f(b - 2\varepsilon) \geq f(b - \varepsilon) > f(b)$, contradicting to our assumption that $f$ is 
        symmetric about b.

        Now suppose that the mode is not unique. Then it is the same case except that there is a region 
        $(x_1, x_2)$ such that $b$ is a mode for all $b \in (x_1, x_2)$.

        \item $f$ is monotonically decreasing on $[0, \infty)$ hence it is unimodal with mode 0.
    \end{enumerate}

    % 2.28
    \item \begin{enumerate}
        \item From part (c) of Exercise 2.26, $\mathbb{E}[X] = a$. Then 
        \begin{align*}
            \mu_3
            &= \int_{-\infty}^{\infty} (x - a)^3 f(x) \ dx \\
            &= \int_{-\infty}^{a} (x - a)^3 f(x) \ dx + \int_{a}^{\infty} (x - a)^3 f(x) \ dx \\
            &= \int_{-\infty}^{0} u^3 f(a + u) \ du + \int_{0}^{\infty} u^3 f(a + u) \ du \quad 
            (u = x - a) \\
            &= \int_{0}^{\infty} (-v)^3 f(a - v) \ dv + \int_{0}^{\infty} u^3 f(a + u) \ du \quad
            (v = -u) \\
            &= -\int_{0}^{\infty} v^3 f(a + v) \ dv + \int_{0}^{\infty} u^3 f(a + u) \ du \quad 
            (f(a - v) = f(a + v)) \\
            &= 0.
        \end{align*}

        \item First of all, 
        \[ E[X] = \int_{0}^{\infty} xe^{-x} \ dx = [-xe^{-x} - e^{-x}]_{0}^{\infty} = 1. \]
        Then 
        \begin{align*}
            \mu_2
            &= \int_{0}^{\infty} (x - 1)^2 e^{-x} \ dx \\
            &= [-(x - 1)^2 e^{-x} - 2(x - 1)e^{-x} - 2e^{-x}]_{0}^{\infty} \\
            &= 0 - (-1 + 2 - 2) \\
            &= 1, \\
            \mu_3
            &= \int_{0}^{\infty} (x - 1)^2 e^{-x} \ dx \\
            &= [-(x - 1)^3 e^{-x} - 3(x - 1)^2 e^{-x} - 6(x - 1)e^{-x} - 6e^{-x}]_{0}^{\infty} \\
            &= 0 - (1 - 3 + 6 - 6) \\
            &= 2.
        \end{align*}
        Therefore $\alpha_3 = \frac{2}{1^{3 / 2}} = 2$.

        \item The first pdf is the standard normal, so for any even number $n = 2k, k \in \mathbb{N}$, 
        $\mathbb{E}[X^n] = (n - 1)!!$ so $\alpha_4 = \frac{3}{1^2} = 3$.

        For the second pdf, 
        \begin{align*}
            &\mathbb{E}[X^2] 
            = \int_{-1}^{1} \frac{1}{2}x^2 \ dx 
            = \Bigl[ \frac{1}{6}x^3 \Bigr]_{-1}^{1} 
            = \frac{1}{3}. \\
            &\mathbb{E}[X^2] 
            = \int_{-1}^{1} \frac{1}{2}x^4 \ dx 
            = \Bigl[ \frac{1}{10}x^5 \Bigr]_{-1}^{1} 
            = \frac{1}{5}. \\
            &\mu_4 
            = \frac{1}{5} / \Bigl( \frac{1}{3} \Bigr)^2 
            = \frac{9}{5}.
        \end{align*}

        For the third pdf, since it is symmetric and unimodal, $\mathbb{E}[X] = 0$. Then 
        \begin{align*}
            &\mathbb{E}[X^2]
            = \int_{-\infty}^{0} \frac{1}{2}x^2 e^x \ dx + \int_{0}^{\infty} \frac{1}{2}x^2 e^{-x} \ dx 
            = 2. \\
            &\mathbb{E}[X^4]
            = \int_{-\infty}^{0} \frac{1}{2}x^4 e^x \ dx + \int_{0}^{\infty} \frac{1}{2}x^4 e^{-x} \ dx 
            = 24. \\
            &\mu_4 
            = \frac{24}{2^2} 
            = 6.
        \end{align*}

        We can see that the larger the kurtosis, the more peaked the pdf is.
    \end{enumerate}

    % 2.29
    \item \begin{enumerate}
        \item For the $\text{Binomial}(n, p)$ distribution, 
        \begin{align*}
            \mathbb{E}[X(X - 1)]
            &= \sum_{k = 0}^{n} {\color{blue} k(k - 1) \binom{n}{k}} p^k (1 - p)^{n - k} \\
            &= \sum_{k = 2}^{n} {\color{blue} n(n - 1) \binom{n - 2}{k - 2}} p^k (1 - p)^{n - k} \\
            &= n(n - 1)p^2 \sum_{l = 0}^{n - 2} \binom{n - 2}{l} p^l (1 - p)^{n - 2 - l} \\
            &= n(n - 1)p^2,
        \end{align*}
        where we used the substitution $l = k - 2$.

        For the $\text{Poisson}(\lambda)$ distribution, 
        \begin{align*}
            \mathbb{E}[X(X - 1)]
            &= \sum_{k = 0}^{\infty} k(k - 1) \frac{\lambda^k e^{-\lambda}}{k!} \\
            &= \sum_{k = 2}^{\infty} k(k - 1) \frac{\lambda^k e^{-\lambda}}{k!} \\
            &= \lambda^2 \sum_{k = 2}^{\infty} \frac{\lambda^{k - 2} e^{-\lambda}}{(k - 2)!} \\
            &= \lambda^2.
        \end{align*}

        \item Since $\var{X} = \mathbb{E}[X(X - 1)] + \mathbb{E}[X] - (\mathbb{E}[X])^2$, for the binomial, 
        \[ \var{X} = n(n - 1)p^2 + np - (np)^2 = np(1 - p). \]
        For the Poisson, 
        \[ \var{X} = \lambda^2 + \lambda - \lambda^2 = \lambda. \]

        \item 
    \end{enumerate}

    % 2.30 
    \item \begin{enumerate}
        \item 
    \end{enumerate}

    % 2.31
    \item No such distribution exists. First note that $M_{X}(0) = \mathbb{E}[e^0] = 1$. If the mgf were 
    to be that stated in the question, $M_X(0) = 0$, which is incorrect.

    % 2.32
    \item \begin{align*}
        \frac{d}{dt} S(t) \Big|_{t = 0}
        = \frac{1}{M_X(t)} \cdot M'_X(t) \Big|_{t = 0}
        = \frac{1}{1} \cdot M'_X(0) 
        = \mathbb{E}[X].
    \end{align*}
    \begin{align*}
        \frac{d^2}{dt^2} S(t) \Big|_{t = 0} 
        &= -\frac{1}{M_X^2(t)} \cdot (M'_X(t))^2 + \frac{1}{M_X(t)} M''_X(t) \Big|_{t = 0} \\
        &= -\frac{1}{1^2} (M'_X(0))^2 + \frac{1}{1} M''_X(0) \\
        &= \mathbb{E}[X^2] - (\mathbb{E}[X])^2 \\
        &= \var{X}.
    \end{align*}

    % 2.33
    \item \begin{enumerate}
        \item \begin{align*}
            M_X(t) 
            &= \sum_{x = 0}^{\infty} e^{tx} \frac{\lambda^x e^{-\lambda}}{x!} \\
            &= e^{-\lambda} \sum_{x = 0}^{\infty} \frac{(\lambda e^t)^x}{x!} \\
            &= e^{-\lambda} \cdot e^{\lambda e^t} \\
            &= e^{\lambda (e^t - 1)}.
        \end{align*}
        Therefore 
        \[ \mathbb{E}[X] = M'_X(0) = \lambda e^t \cdot e^{\lambda (e^t - 1)} |_{t = 0} = \lambda, \]
        \[ \mathbb{E}[X^2] = M''_X(0) = \lambda e^t \cdot e^{\lambda (e^t - 1)} 
        + (\lambda e^t)^2 \cdot e^{\lambda (e^t - 1)} = \lambda^2 + \lambda, \]
        \[ \var{X} = \lambda^2 + \lambda - \lambda^2 = \lambda. \]

        \item \begin{align*}
            M_X(t)
            &= \sum_{x = 0}^{\infty} e^{tx} \cdot p(1 - p)^x \\
            &= p \sum_{x = 0}^{\infty} ((1 - p)e^t)^x \\
            &= \frac{p}{1 - (1 - p)e^t}, \ t < -\log(1 - p).
        \end{align*}
        Therefore 
        \[ \mathbb{E}[X] = M'_X(0) = \]
    \end{enumerate}
\end{enumerate}

\end{document}