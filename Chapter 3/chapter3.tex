\documentclass{article}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage[margin=1.5in]{geometry}

\DeclareMathOperator{\var}{Var}
\DeclareMathOperator{\corr}{Corr}

\title{Statistical Inference Chapter 3}
\author{Gallant Tsao}

\begin{document}

\maketitle

\begin{enumerate}
    % 3.1
    \item We first note that the pmf of $X$ is 
    \[ p_X(x) = \frac{1}{N_1 - N_0 + 1}, \ x \in \{N_0, N_0 + 1, ..., N_1\}. \]
    Then we get the expectation to be 
    \begin{align*}
        \mathbb{E}[X]
        &= \sum_{x = N_0}^{N_1} x\frac{1}{N_1 - N_0 + 1} \\
        &= \frac{1}{N_1 - N_0 + 1} \cdot \frac{N_1 - N_0 + 1}{2} (2N_0 + (N_1 - N_0 + 1 - 1)) \\
        &= \frac{N_1 + N_0}{2}.
    \end{align*}
    As for the variance, we get 
    \begin{align*}
        \mathbb{E}[X^2]
        &= \sum_{x = N_0}^{N_1} x^2 \frac{1}{N_1 - N_0 + 1} \\ 
        &= \frac{1}{N_1 - N_0 + 1} \Big( \sum_{x = 1}^{N_1}x^2 - \sum_{x = 1}^{N_0 - 1} x^2 \Big) \\
        &= \frac{1}{N_1 - N_0 + 1} \Big( \frac{N_1(N_1 + 1)(N_1 + 2) - (N_0 - 1)(N_0)(2N_0 - 1)}{6} \Big) \\
    \end{align*}
    So that 
    \begin{align*}
        Var(X)
        &= \mathbb{E}[X^2] - (\mathbb{E}[X])^2 \\
        &= 1
    \end{align*}

    % 3.2
    \item Let $X = $ number of defective parts in the sample. Then \\
    $X \sim \text{Hypergeometric}(100, n, K)$.
    \begin{enumerate}
        \item Firstly, we need $n = 6$ because for the same $K$, increasing $n$ decreases the 
        value of the Hypergeometric pmf (image shown at end of answer). Then with $n = 6$, 
        \begin{align*}
            P(X = 0) 
            &= \frac{\binom{6}{0}\binom{94}{K}}{\binom{100}{K}} \\
            &= \frac{(100 - k)\cdots (100 - K - 5)}{100 \cdots 95}.
        \end{align*}
        After some trial and error with the calculations, we have that when 
        $K = 31$, $P(X = 0) = 0.10056$, but when $K = 32$, $P(X = 0) = 0.09182$. Therefore, 
        the sample size must be at least 32.
        \begin{center}
            \includegraphics*[width=0.8\textwidth]{../scripts/3-2.png}
        \end{center}
        
        \item By the same reasoning above, we need $n = 6$. Then with this $n$,
        \begin{align*}
            P(X = 0 \text{ or } 1) 
            &= \frac{\binom{6}{0}\binom{94}{K}}{\binom{100}{K}} 
            + \frac{\binom{6}{1}\binom{94}{K - 1}}{\binom{100}{K}}.
        \end{align*}
        Again, by trial and error, when $K = 50$, $P(X = 0 \text{ or } 1) = 0.10220$, but when 
        $K = 51$, $P(X = 0 \text{ or } 1) =0.09331$ hence the sample size must be at least 51.
    \end{enumerate}

    % 3.3
    \item During the three seconds that the person is crossing, there should be no cars passing. 
    The probability of this happening is $(1 - p)^3$. The only possibility for the person to 
    not wait exactly 4 seconds is when there is a car at the first second and no cars in the 
    next 3 seconds. The probability of this happening is $p(1 - p)^3$. Since the times are 
    independent, the probability that the pedestrian has to wait exactly 4 seconds is 
    $[1 - p(1 - p)^3] (1 - p)^3$.

    % 3.4
    \item \begin{enumerate}
        \item Let $X$ be the number of trials. Then in this case $X \sim \text{Geom}(0.1)$. 
        Therefore the mean number of trials is just $\frac{1}{0.1} = 10$.

        \item 
    \end{enumerate}

    % 3.5
    \item Let $X = $ number of effective cases. Suppose the new drug is equally effective as 
    the old drug. Then $X \sim \text{Binomial}(100, 0.8)$ if the cases are independent from each 
    other, which is a reasonable assumption. We have 
    \[ P(X \geq 85) = \sum_{k = 85}^{100}\binom{100}{k} 0.8^{k} \cdot 0.2^{100 - k} = 0.1285. \]
    From this, the probability of getting 85 or more effective cases is not too small, hence we 
    cannot directly make a conclusion that the new drug is superior.

    % 3.6
    \item \begin{enumerate}
        \item $X \sim \text{Binomial}(2000, 0.01)$.

        \item \[ \sum_{k = 0}^{99} \binom{2000}{k} 0.01^{k} \cdot 0.99^{2000 - k}. \]

        \item In our problem, $n = 2000, p = 0.01, q = 0.99$. Since $np, nq > 5$, we can use 
        normal approximation here. The normal approximation is $Y \sim N(\mu, \sigma^2)$, where 
        \[ \mu = np = 20, \sigma^2 = npq = 19.8. \]
        Then we get 
        \[ P(X < 100) \approx P(Z < 17.979) = 1. \]
    \end{enumerate}

    % 3.7
    \item Let $X$ be the number of chocolate chips in the cookie. Then 
    $X \sim \text{Poisson}(\lambda)$. We want that 
    \[ P(X \geq 2) = 1 - P(X \leq 1) > 0.99 \implies P(X \leq 1) 
    = e^{-\lambda} + \lambda e^{-\lambda} < 0.01. \]
    Solving the above numerically, we get that $\lambda = 6.6384.$

    % 3.8
    \item \begin{enumerate}
        \item Let $X$ be the number of customers in the theater. Then 
        $X \sim \text{Binomial}(1000, \frac{1}{2}).$ We want 
        \[ P(X > N) = \sum_{k = N + 1}^{1000} \binom{1000}{k} \Big( \frac{1}{2} \Big)^{k} 
        \Big( 1 - \frac{1}{2} \Big)^{1000 - k} < 0.01.\]
        In other words, we are solving the smallest $N$ such that 
        \[ \Big( \frac{1}{2} \Big)^{1000} \sum_{k = N + 1}^{1000} \binom{1000}{k} < 0.01. \]
        By looping over $N$, we eventually get that $N = 537$.

        \item $n = 1000, p = q = \frac{1}{2}$. Therefore the parameters for the normal 
        approximation are $\mu = np = 500, \sigma^2 = npq = 250$. Then we are solving for 
        \[ P(X > N) \approx P(Z > \frac{N - 500}{\sqrt{250}}) < 0.01. \]
        Using R, we get that 
        \[ \frac{N - 500}{\sqrt{250}} = 2.326 \implies N \approx 537, \]
        which is the same as our answer in part (a).
    \end{enumerate} 

    % 3.9
    \item \begin{enumerate}
        \item Let $X \sim \text{Binomial}$ as depicted in the question. 
        \begin{align*}
            P(X \geq 5)
            &= 1 - P(X \leq 4) \\
            &= 1 - \sum_{k = 0}^{4} \binom{60}{k} \Big( \frac{1}{90} \Big)^{k} 
            \Bigl( 1 - \frac{1}{90} \Bigr)^{60 - k} \\
            &\approx 0.0006,
        \end{align*}
        which I think is rare enough to be on the news.

        \item Let $X$ be the number of schools in New York state with 5 or more sets of twins. Then 
        $X \sim \text{Binomial}(360, 0.0006)$. We have that 
        \[ P(X \geq 1) = 1 - P(X = 0) \approx 0.1698. \]

        \item Let $X$ be the number of states in the past 10 years having 5 or more sets of twins. Then 
        $X \sim \text{Binomial}(500, 0.1698)$. We have that 
        \[ P(X \geq 1) = 1 - P(X = 0) = 1. \]
        Therefore this event becomes almost certain as we broaden the time scope.
    \end{enumerate}

    % 3.10
    \item \begin{enumerate}
        \item Let $X$ be the number of packets of cocaine from the first draw, and let $Y$ be the number of 
        noncocaine packets from the second draw. Then we have that $X \sim \text{Hypergeometric}
        (N + M, N, 4)$ and $Y \sim \text{Hypergeometric}(N + M - 4, M, 2).$ Then the probability that the 
        defendant is innocent is 
        \[ P(X = 4)P(Y = 2) = \frac{\binom{N}{4}\binom{M}{0}}{\binom{N + M}{4}} 
        \frac{\binom{M}{2}\binom{N - 4}{0}}{\binom{N + M - 4}{2}} 
        = \frac{\binom{N}{4}\binom{M}{2}}{\binom{N + M}{4}\binom{N + M - 4}{2}}. \]

        \item Since the denominator from part (a) is a constant, we just have to find the maximizer of the 
        numerator, which is just $\binom{N}{4}\binom{496 - N}{2}.$ After some calculus, the local maximizer 
        is about $330.834$, hence the maximum is attained at $N = 331, M = 165$, with value about 0.022.
    \end{enumerate}

    % 3.11
    \item \begin{enumerate}
        \item 
    \end{enumerate}

    % 3.12
    \item Consider a sequence of independent $\text{Bernoulli}(p)$ random variables. We define $X = $
    Number of successes in $n$ trials, and $Y = $ Number of failures until the $r$th success. Then 
    $X, Y$ have the specified distributions in the questions. Then
    \begin{align*}
        F_{X}(r - 1)
        &= P(X \leq r - 1) \\
        &= P(r\text{th success on } (n + 1)\text{th or later trial}) \\
        &= P(\text{At least } (n + 1 - r) \text{ failures before the }r \text{ th success}) \\
        &= P(Y \geq n - r + 1) \\
        &= 1 - P(Y \leq n - r) \\
        &= 1 - F_{Y}(n - r).
    \end{align*}

    % 3.13
    \item Firstly, note that we can find the expectation and variance of the truncated distribution for a 
    general discrete random variable ranging from 0, then we can plug in the values:
    \begin{align*}
        \mathbb{E}[X_{T}]
        &= \sum_{k = 1}^{\infty} kP(X_{T} = k) \\
        &= \sum_{k = 1}^{\infty} k\frac{P(X = k)}{P(X > 0)} \\
        &= \frac{1}{P(X > 0)} \sum_{k = 1}^{\infty} kP(X = k) \\
        &= \frac{\mathbb{E}[X]}{P(X > 0)}.
    \end{align*}
    From the same way we get that 
    \[ \mathbb{E}[X_{T}^2] = \frac{\mathbb{E}[X^2]}{P(X > 0)}. \]
    Therefore, 
    \[ \var{X_{T}} = \frac{\mathbb{E}[X^2]}{P(X > 0)} - \Bigl( \frac{\mathbb{E}[X]}{P(X > 0)} \Bigr)^2. \]

    \begin{enumerate}
        \item For $\text{Poisson}(\lambda)$, $P(X > 0) = 1 - e^{-\lambda}$.
        \[ P(X_{T} = k) = \frac{\lambda^{k} e^{-\lambda}}{k!(1 - e^{-\lambda})}, \ k = 1, 2, \dots \]
        and therefore 
        \[ \mathbb{E}[X_{T}] = \frac{\lambda}{1 - e^{-\lambda}}, 
        \var{X_{T}} = \frac{\lambda^2 + \lambda}{1 - e^{-\lambda}} 
        - \Bigl( \frac{\lambda}{1 - e^{-\lambda}}\Bigr)^2. \]

        \item For $\text{NB}(r, p)$, $P(X > 0) = 1 - \binom{r - 1}{0} (1 - p)^{k}p^{r} = 1 - p^{r}$.
        \[ P(X_{T} = k) = \frac{\binom{k + r - 1}{k}(1 - p)^{k} p^{r}}{1 - p^{r}}, \ k = 1, 2, \dots \]
        and therefore 
        \[ \mathbb{E}[X_{T}] = \frac{r(1 - p)}{p(1 - p^{r})}, 
        \var{X_{T}} = \frac{r(1 - p) + r^{2}(1 - p)^{2}}{p^{2}(1 - p^{r})} 
        - \Bigl( \frac{r(1 - p)}{p(1 - p^{r})} \Bigr)^{2}. \]
    \end{enumerate}

    % 3.14
    \item \begin{enumerate}
        \item \begin{align*}
            \sum_{x = 1}^{\infty} \frac{-(1 - p)^{x}}{x\log{p}} 
            &= \frac{1}{\log{p}} \sum_{x = 1}^{\infty} \frac{-(1 - p)^{x}}{x} \\
            &= \frac{1}{\log{p}} \cdot \log{p} \\
            &= 1
        \end{align*}
        as the latter term is the Taylor series for $\log{p}$.

        \item \begin{align*}
            \mathbb{E}[X]
            &= \sum_{x = 1}^{\infty} x \cdot \frac{-(1 - p)^{x}}{x\log{p}} \\
            &= -\frac{1}{\log{p}} \sum_{x = 1}^{\infty} (1 - p)^{x} \\
            &= -\frac{1 - p}{p\log{p}}.
        \end{align*}
        Also, \begin{align*}
            \mathbb{E}[X^2]
            &= -\frac{1}{\log{p}} \sum_{x = 1}^{\infty} x(1 - p)^{x} \\
            &= \frac{1 - p}{\log{p}} \sum_{x = 1}^{\infty} \frac{d}{dp} (1 - p)^{x} \\
            &= \frac{1 - p}{\log{p}} \frac{d}{dp} \sum_{x = 1}^{\infty} (1 - p)^{x} \\
            &= \frac{1 - p}{\log{p}} \frac{d}{dp} \Bigl( \frac{1 - p}{p} \Bigr) \\
            &= \frac{-(1 - p)}{p^{2}\log{p}}.
        \end{align*}
        Therefore 
        \[ \var{X} = \mathbb{E}[X^2] - (\mathbb{E}[X])^2 = \frac{-(1 - p)}{p^{2}\log{p}} 
        \Bigl[ 1 + \frac{1 - p}{\log{p}} \Bigr]. \]
    \end{enumerate}

    % 3.15
    \item The moment generating function for $\text{NB}(r, p)$ is (after some algebraic manipulations)
    \begin{align*}
        M(t) 
        &= \Bigl( \frac{p}{1 - (1 - p)e^{t}} \Bigr)^{r}, \ t < -\log{(1 - p)} \\
        &= \Bigl( \frac{1 - (1 - p)e^{t}}{1 - (1 - p)e^{t}} 
        + \frac{(1 - p)(e^{t} - 1)}{1 - (1 - p)e^{t}} \Bigr)^{r} \\
        &= \Bigl( 1 + \frac{1}{r} \frac{r(1 - p)(e^{t} - 1)}{1 - (1 - p)e^{t}} \Bigr)^{r}.
    \end{align*}
    From above, 
    \[ \frac{r(1 - p)(e^{t} - 1)}{1 - (1 - p)e^{t}} \to \frac{\lambda (e^{t} - 1)}{1} 
    = \lambda (e^{t} - 1) \text{ as } r \to \infty, \ p \to 1, \text{ and } r(1 - p) \to \lambda. \]
    Therefore, taking the limit, 
    \[ M(t) \to \lim_{r \to \infty} \Bigl( 1 + \frac{\lambda (e^{t} - 1)}{r} \Bigr)^{r} 
    = e^{\lambda(e^{t} - 1)}, \]
    which is exactly the moment generating function of the Poisson random variable.

    % 3.16
    \item \begin{enumerate}
        \item \begin{align*}
            \Gamma(\alpha + 1)
            &= \int_{0}^{\infty} t^{\alpha} e^{-t} \ dt \\
            &= [-t^{\alpha} e^{-t}]_{0}^{\infty} + \alpha \int_{0}^{\infty} t^{\alpha - 1} e^{-t} \ dt \\
            &= \alpha \Gamma(\alpha).
        \end{align*}

        \item \begin{align*}
            \Gamma(\frac{1}{2})
            &= \int_{0}^{\infty} t^{-\frac{1}{2}} e^{-t} \ dt \\
            &= \int_{0}^{\infty} 2e^{-u^2} \ du \\
            &= 2 \cdot \frac{\sqrt{\pi}}{2} \\
            &= \sqrt{\pi}.
        \end{align*}
    \end{enumerate}

    % 3.17
    \item \begin{align*}
        \mathbb{E}[X^{\nu}]
        &= \int_{0}^{\infty} x^{\nu} \cdot \frac{1}{\beta^{\alpha} \Gamma(\alpha)} 
        x^{\alpha - 1} e^{-x/\beta} \ dx \\
        &= \frac{1}{\beta^{\alpha} \Gamma(\alpha)} \int_{0}^{\infty} 
        x^{\nu + \alpha - 1} e^{-x/\beta} \ dx \\
        &= \frac{\beta^{\nu + \alpha} \Gamma(\nu + \alpha)}{\beta^{\alpha} \Gamma(\alpha)} \\
        &= \frac{\beta^{\nu} \Gamma(\nu + \alpha)}{\Gamma(\alpha)}.
    \end{align*}

    % 3.18
    \item The moment generating function of a $\text{NB}(r, p)$ random variable is 
    \[ M_{Y}(t) = \Bigl( \frac{p}{1 - (1 - p)e^{t}} \Bigr)^{r}, \ t < -\log{(1 - p)}. \]
    Then from Theorem 2.3.15, 
    \[ M_{pY}(t) = M_{Y}(pt) = \Bigl( \frac{p}{1 - (1 - p)e^{pt}} \Bigr)^{r}. \]
    Taking $p \to 0$, the above is of the form $\frac{0}{0}$. Then by L'Hopital's Rule, 
    \[ \lim_{p \to 0} \frac{p}{1 - (1 - p)e^{pt}} 
    = \lim_{p \to 0} \frac{1}{(p - 1)te^{pt} + e^{pt}} = \frac{1}{1 - t}. \]
    Therefore, 
    \[ \lim_{p \to 0} M_{pY}(t) = \Bigl( \frac{1}{1 - t} \Bigr)^{r} = (1 - t)^{-r}, \]
    which is exactly the moment generating function of a $\text{Gamma}(r, 1)$ random variable.

    % 3.19
    \item Since $\alpha \in \mathbb{N}$, $\Gamma(\alpha) = (\alpha - 1)!$. Then 
    \begin{align*}
        \frac{1}{\Gamma(\alpha)} \int_{x}^{\infty} z^{\alpha - 1} e^{-z} \ dz
        &= \Bigl[ -\frac{z^{\alpha - 1}e^{-z}}{(\alpha - 1)!} \Bigr]_{x}^{\infty} 
        + \frac{\alpha - 1}{(\alpha - 1)!} \int_{x}^{\infty} z^{\alpha - 2}e^{-z} \ dz \\
        &= \frac{x^{\alpha - 1}e^{-x}}{(\alpha - 1)!} 
        + \frac{1}{(\alpha - 2)!} \int_{x}^{\infty} z^{\alpha - 2}e^{-z} \ dz \\
        &= \cdots \\
        &= \sum_{y = 0}^{\alpha - 1} \frac{x^{y}e^{-x}}{y!}, \ \alpha = 1, 2, 3, \dots
    \end{align*}

    % 3.20
    \item \begin{enumerate}
        \item \begin{align*}
            \mathbb{E}[X]
            &= \int_{0}^{\infty} \frac{2}{\sqrt{2\pi}} xe^{-x^2 / 2} \ dx \\
            &= \frac{2}{\sqrt{2\pi}} \int_{0}^{\infty} e^{-u} \ du \quad (u = x^2 / 2) \\
            &= \frac{2}{\sqrt{2\pi}}, \\
            \mathbb{E}[X^2]
            &= \int_{0}^{\infty} \frac{2}{\sqrt{2\pi}} x^2 e^{-x^2 / 2} \ dx \\
            &= \Bigl[ -\frac{2}{\sqrt{2\pi}} xe^{-x^2 / 2} \Bigr]_{0}^{\infty} 
            + \frac{2}{\sqrt{2\pi}} \int_{0}^{\infty} e^{-x^2 / 2} \ dx \\
            &= \frac{1}{\sqrt{2}}, \\
            \var{X}
            &= \frac{1}{\sqrt{2}} - \Bigl( \frac{2}{\sqrt{2\pi}} \Bigr)^2 \\
            &= \frac{1}{\sqrt{2}} - \frac{1}{\pi}.
        \end{align*}

        \item Let $Z$ be a standard normal random variable. Since $X = |Z|$, $X^2 = Z^2 \sim \chi_{1}^2$. 
        More importantly, $\chi_{1}^2 \sim \text{Gamma}(\frac{1}{2}, 2)$. Therefore the transformation we 
        need is just $g(x) = x^2$ and $\alpha = \frac{1}{2}, \beta = 2$.
    \end{enumerate}

    % 3.21
    \item The integral for the mgf is $\frac{1}{\pi} \int_{\infty}^{\infty} \frac{e^{tx}}{1 + x^2} \ dx$. 
    Note that on $(0, \infty)$, $e^{tx} > x$ and therefore 
    \[ \int_{0}^{\infty} \frac{e^{tx}}{1 + x^2} \ dx > \int_{0}^{\infty} \frac{x}{1 + x^2} \ dx = \infty \]
    so the integral is not finite.

    % 3.22
    \item \begin{enumerate}
        \item 
    \end{enumerate}

    % 3.23
    \item \begin{enumerate}
        \item \[ 
        \int_{\alpha}^{\infty} \frac{\beta \alpha^{\beta}}{x^{\beta + 1}} \ dx 
        = \Bigl[ -\frac{\alpha^{\beta}}{x^{\beta}} \Bigr]_{\alpha}^{\infty} = 0 - (-1) = 1. 
        \]

        \item First note that 
        \begin{align*}
            \mathbb{E}[X^{n}]
            &= \int_{\alpha}^{\infty} \frac{\beta \alpha^{\beta}}{x^{\beta - n + 1}} \ dx \\
            &= \Bigl[ \frac{\beta \alpha^{\beta}}{(\beta - n)x^{\beta - n}} \Bigr]_{\alpha}^{\infty} \\
            &= \frac{\alpha^{n} \beta}{n - \beta}.
        \end{align*}
        Therefore \begin{align*}
            \mathbb{E}[X] &= \frac{\alpha \beta}{1 - \beta}, \\
            \mathbb{E}[X^2] &= \frac{\alpha^{2} \beta}{2 - \beta}, \\
            \var{X} &= \frac{\alpha^{2} \beta}{2 - \beta} - \Bigl( \frac{\alpha \beta}{1 - \beta} \Bigr)^2.
        \end{align*}

        \item If $\beta \leq 2$, 
        \[ \mathbb{E}[X^2] 
        = \int_{\alpha}^{\infty} \frac{\beta \alpha^{\beta}}{x^{1 - \beta}} 
        > \int_{\alpha}^{\infty} \frac{\beta \alpha^{\beta}}{x} 
        = \infty \]
        Hence the integral diverges so the second moment and variance do not exist.
    \end{enumerate}

    % 3.24
    \item \begin{enumerate}
        \item Since $f_{X}(x) = \frac{1}{\beta}e^{-x / \beta}, \ x > 0$, we get that 
        $f{Y}(y) = \frac{\gamma}{\beta} y^{\gamma - 1} e^{-y^\gamma / \beta}, \ y > 0$.
    \end{enumerate}

    % 3.25
    \item \begin{align*}
        h_{T}(t) 
        &= \lim_{\delta \to 0} \frac{P(t \leq T < t + \delta | T \geq t)}{\delta} \\
        &= \lim_{\delta \to 0} \frac{P(t \leq T < t + \delta, T \geq t)}{\delta P(T \geq t)} \\
        &= \lim_{\delta \to 0} \frac{P(t \leq T < t + \delta)}{\delta (1 - F_{T}(t))} \\
        &= \frac{1}{1 - F_{T}(t)} \lim_{\delta \to 0} \frac{F_{T}(t + \delta) - F_{T}(t)}{\delta} \\
        &= \frac{f_{T}(t)}{1 - F_{T}(t)}.
    \end{align*}
    The second equality is clear by some calculations.

    % 3.26
    \item \begin{enumerate}
        \item Since $T \sim \text{Exp}(\beta)$, $F_{T}(t) = 1 - e^{-t / \beta}$. Then 
        \begin{align*}
            h_{T}(t)
            &= -\frac{d}{dt} \log(1 - (1 - e^{-t / \beta})) \\
            &= -\frac{d}{dt} \Bigl( -\frac{t}{\beta} \Bigr) \\
            &= \frac{1}{\beta}.
        \end{align*}

        \item Since $T \sim \text{Weibull}(\gamma, \beta)$, $F_{T}(t) = $
    \end{enumerate}

    % 3.27
    \item \begin{enumerate}
        \item 
    \end{enumerate}
\end{enumerate}

\end{document}